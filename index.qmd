---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Garbage Can Regression Challenge

**Choose R or Python and delete the other code chunk.**

## R Code

```{r}
#| echo: true
library(tidyverse)
library(broom)

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
observDF <- tribble(
  ~Stress, ~StressSurvey, ~Time, ~Anxiety,
  0,0,0,0,
  0,0,1,0.1,
  0,0,1,0.1,
  1,3,1,1.1,
  1,3,1,1.1,
  1,3,1,1.1,
  2,6,2,2.2,
  2,6,2,2.2,
  2,6,2,2.2,
  8,9,2,8.2,
  8,9,2,8.2,
  8,9,2.1,8.21,
  12,12,2.2,12.22,
  12,12,2.2,12.22,
  12,12,2.2,12.22
)

observDF
```

```{r}
observDF %>%
ggplot(aes(x = Stress, y = StressSurvey)) +
geom_line(linewidth = 1, color = "purple")+
geom_point(size = 6, color = "purple") +
labs(
title = "StressSurvey seems a decent (monotonic) proxy for actual Stress",
x = "Actual Stress Level",
y = "Stress Survey Response"
) +
theme_minimal()
```
## Your Analysis

Follow the challenge instructions from your course to complete your analysis.
## Question 1

Run a bivariate regression of Anxiety on StressSurvey. What are the estimated coefficients? How do they compare to the true relationship?

```{r}
# Bivariate regression: Anxiety ~ StressSurvey
model1 <- lm(Anxiety ~ StressSurvey, data = observDF)

# Display the regression results
summary(model1)

# Extract coefficients for comparison
coef_model1 <- coef(model1)
cat("Estimated coefficients:\n")
cat("Intercept:", round(coef_model1[1], 4), "\n")
cat("StressSurvey coefficient:", round(coef_model1[2], 4), "\n")

# True relationship: Anxiety = Stress + 0.1 × Time
# Since StressSurvey is a proxy for Stress, let's see how it compares
cat("\nTrue relationship: Anxiety = Stress + 0.1 × Time\n")
cat("Since StressSurvey is a proxy for Stress, we expect the coefficient to be close to 1\n")
```

**My takeaways here is that the coefficient is very close to true value (1.047 vs 1.0). StressSurvey captures 90% of the true relationship with minimal bias**

## Question 2

Create a scatter plot with the regression line showing the relationship between StressSurvey and Anxiety. Comment on the fit and any potential issues.

```{r}
# Create scatter plot with regression line
ggplot(observDF, aes(x = StressSurvey, y = Anxiety)) +
  geom_point(size = 4, alpha = 0.7, color = "darkblue") +
  geom_smooth(method = "lm", se = TRUE, color = "red", linewidth = 1.5) +
  labs(
    title = "Bivariate Relationship: StressSurvey vs Anxiety",
    subtitle = "Scatter plot with regression line and confidence interval",
    x = "Stress Survey Response",
    y = "Anxiety Level",
    caption = "Red line shows linear regression fit with 95% confidence interval"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "gray60"),
    axis.title = element_text(size = 11)
  )

# Calculate and display fit statistics
cat("Fit Statistics:\n")
cat("R-squared:", round(summary(model1)$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary(model1)$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary(model1)$sigma, 4), "\n")

# Check for potential issues
cat("\nPotential Issues Analysis:\n")
cat("1. Residuals range:", round(range(residuals(model1)), 3), "\n")
cat("2. Cook's Distance max:", round(max(cooks.distance(model1)), 4), "\n")
cat("3. Leverage max:", round(max(hatvalues(model1)), 4), "\n")

# Check for outliers using studentized residuals
studentized_residuals <- rstudent(model1)
outliers <- abs(studentized_residuals) > 2
cat("4. Potential outliers (|studentized residual| > 2):", sum(outliers), "observations\n")
if(sum(outliers) > 0) {
  cat("   Outlier observations:", which(outliers), "\n")
}
```

**The scatter plot shows a strong, tight linear relationship between StressSurvey and Anxiety. This shows confirmation that StressSurvey captures the underlying stress-anxiety relationship well.**

## Question 3

Run a bivariate regression of Anxiety on Time. What are the estimated coefficients? How do they compare to the true relationship?

```{r}
# Bivariate regression: Anxiety ~ Time
model2 <- lm(Anxiety ~ Time, data = observDF)

# Display the regression results
summary(model2)

# Extract coefficients for comparison
coef_model2 <- coef(model2)
cat("Estimated coefficients:\n")
cat("Intercept:", round(coef_model2[1], 4), "\n")
cat("Time coefficient:", round(coef_model2[2], 4), "\n")

# True relationship: Anxiety = Stress + 0.1 × Time
# The Time coefficient should be 0.1
cat("\nTrue relationship: Anxiety = Stress + 0.1 × Time\n")
cat("Expected Time coefficient: 0.1\n")
cat("Estimated Time coefficient:", round(coef_model2[2], 4), "\n")
cat("Difference from true coefficient:", round(coef_model2[2] - 0.1, 4), "\n")

# Calculate R-squared and other fit statistics
cat("\nModel fit statistics:\n")
cat("R-squared:", round(summary(model2)$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary(model2)$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary(model2)$sigma, 4), "\n")

# Compare with the true relationship
cat("\nComparison with true relationship:\n")
cat("True model: Anxiety = Stress + 0.1 × Time\n")
cat("Estimated model: Anxiety =", round(coef_model2[1], 4), "+", round(coef_model2[2], 4), "× Time\n")
cat("Note: The estimated model ignores the Stress component, which explains the poor fit\n")
```

**This demonstrates severe omitted variable bias - the Time coefficient is 5,341% too high (5.341 vs 0.1) because it absorbs the effect of the missing Stress variable. The Time-only model gives a completely misleading picture of the true relationship.**

## Question 4

Create a scatter plot with the regression line showing the relationship between Time and Anxiety. Comment on the fit and any potential issues.

```{r}
# Create scatter plot with regression line for Time vs Anxiety
ggplot(observDF, aes(x = Time, y = Anxiety)) +
  geom_point(size = 4, alpha = 0.7, color = "darkgreen") +
  geom_smooth(method = "lm", se = TRUE, color = "red", linewidth = 1.5) +
  labs(
    title = "Bivariate Relationship: Time vs Anxiety",
    subtitle = "Scatter plot with regression line and confidence interval",
    x = "Time",
    y = "Anxiety Level",
    caption = "Red line shows linear regression fit with 95% confidence interval"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "gray60"),
    axis.title = element_text(size = 11)
  )

# Calculate and display fit statistics for Time model
cat("Fit Statistics for Time Model:\n")
cat("R-squared:", round(summary(model2)$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary(model2)$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary(model2)$sigma, 4), "\n")

# Check for potential issues with Time model
cat("\nPotential Issues Analysis for Time Model:\n")
cat("1. Residuals range:", round(range(residuals(model2)), 3), "\n")
cat("2. Cook's Distance max:", round(max(cooks.distance(model2)), 4), "\n")
cat("3. Leverage max:", round(max(hatvalues(model2)), 4), "\n")

# Check for outliers using studentized residuals
studentized_residuals_time <- rstudent(model2)
outliers_time <- abs(studentized_residuals_time) > 2
cat("4. Potential outliers (|studentized residual| > 2):", sum(outliers_time), "observations\n")
if(sum(outliers_time) > 0) {
  cat("   Outlier observations:", which(outliers_time), "\n")
}

# Compare with StressSurvey model
cat("\nComparison with StressSurvey Model:\n")
cat("StressSurvey model R-squared:", round(summary(model1)$r.squared, 4), "\n")
cat("Time model R-squared:", round(summary(model2)$r.squared, 4), "\n")
cat("Difference in R-squared:", round(summary(model1)$r.squared - summary(model2)$r.squared, 4), "\n")
cat("The Time model explains", round((summary(model2)$r.squared/summary(model1)$r.squared)*100, 1), "% of what the StressSurvey model explains\n")
```

**The visualization clearly demonstrates why using Time alone as a predictor is problematic compared to using StressSurvey. The wider scatter and confidence bands provide visual confirmation of the statistical issues we identified, namely omitted variable bias and poor model specification.**

## Question 5

Run a multiple regression of Anxiety on both StressSurvey and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{r}
# Multiple regression: Anxiety ~ StressSurvey + Time
model3 <- lm(Anxiety ~ StressSurvey + Time, data = observDF)

# Display the regression results
summary(model3)

# Extract coefficients for comparison
coef_model3 <- coef(model3)
cat("Estimated coefficients:\n")
cat("Intercept:", round(coef_model3[1], 4), "\n")
cat("StressSurvey coefficient:", round(coef_model3[2], 4), "\n")
cat("Time coefficient:", round(coef_model3[3], 4), "\n")

# True relationship: Anxiety = Stress + 0.1 × Time
# Since StressSurvey is a proxy for Stress, we expect:
# - StressSurvey coefficient close to 1
# - Time coefficient close to 0.1
cat("\nTrue relationship: Anxiety = Stress + 0.1 × Time\n")
cat("Expected coefficients:\n")
cat("  StressSurvey coefficient: ~1.0 (proxy for Stress)\n")
cat("  Time coefficient: ~0.1\n")

# Compare estimated vs expected coefficients
cat("\nComparison with true relationship:\n")
cat("StressSurvey coefficient - Expected: 1.0, Estimated:", round(coef_model3[2], 4), ", Difference:", round(coef_model3[2] - 1.0, 4), "\n")
cat("Time coefficient - Expected: 0.1, Estimated:", round(coef_model3[3], 4), ", Difference:", round(coef_model3[3] - 0.1, 4), "\n")

# Calculate R-squared and other fit statistics
cat("\nModel fit statistics:\n")
cat("R-squared:", round(summary(model3)$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary(model3)$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary(model3)$sigma, 4), "\n")

# Compare with previous models
cat("\nComparison with previous models:\n")
cat("Bivariate StressSurvey model R-squared:", round(summary(model1)$r.squared, 4), "\n")
cat("Bivariate Time model R-squared:", round(summary(model2)$r.squared, 4), "\n")
cat("Multiple regression R-squared:", round(summary(model3)$r.squared, 4), "\n")
cat("Improvement over StressSurvey model:", round(summary(model3)$r.squared - summary(model1)$r.squared, 4), "\n")
cat("Improvement over Time model:", round(summary(model3)$r.squared - summary(model2)$r.squared, 4), "\n")

# Check for potential issues
cat("\nPotential Issues Analysis:\n")
cat("1. Residuals range:", round(range(residuals(model3)), 3), "\n")
cat("2. Cook's Distance max:", round(max(cooks.distance(model3)), 4), "\n")
cat("3. Leverage max:", round(max(hatvalues(model3)), 4), "\n")

# Check for outliers
studentized_residuals_multiple <- rstudent(model3)
outliers_multiple <- abs(studentized_residuals_multiple) > 2
cat("4. Potential outliers (|studentized residual| > 2):", sum(outliers_multiple), "observations\n")
if(sum(outliers_multiple) > 0) {
  cat("   Outlier observations:", which(outliers_multiple), "\n")
}
```

**This demonstrates the "Garbage Can Regression" problem - the multiple regression achieves the highest R-squared (93.5%) but produces severely biased and wrong-signed coefficients due to multicollinearity. Despite impressive-looking statistics, the model gives misleading results that contradict the true relationship.**

## Question 6

Run a multiple regression of Anxiety on both Stress and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{r}
# Multiple regression: Anxiety ~ Stress + Time
model4 <- lm(Anxiety ~ Stress + Time, data = observDF)

# Display the regression results
summary(model4)

# Extract coefficients for comparison
coef_model4 <- coef(model4)
cat("Estimated coefficients:\n")
cat("Intercept:", round(coef_model4[1], 4), "\n")
cat("Stress coefficient:", round(coef_model4[2], 4), "\n")
cat("Time coefficient:", round(coef_model4[3], 4), "\n")

# True relationship: Anxiety = Stress + 0.1 × Time
cat("\nTrue relationship: Anxiety = Stress + 0.1 × Time\n")
cat("Expected coefficients:\n")
cat("  Stress coefficient: 1.0\n")
cat("  Time coefficient: 0.1\n")

# Compare estimated vs expected coefficients
cat("\nComparison with true relationship:\n")
cat("Stress coefficient - Expected: 1.0, Estimated:", round(coef_model4[2], 4), ", Difference:", round(coef_model4[2] - 1.0, 4), "\n")
cat("Time coefficient - Expected: 0.1, Estimated:", round(coef_model4[3], 4), ", Difference:", round(coef_model4[3] - 0.1, 4), "\n")

# Calculate R-squared and other fit statistics
cat("\nModel fit statistics:\n")
cat("R-squared:", round(summary(model4)$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary(model4)$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary(model4)$sigma, 4), "\n")

# Compare with previous models
cat("\nComparison with previous models:\n")
cat("Bivariate StressSurvey model R-squared:", round(summary(model1)$r.squared, 4), "\n")
cat("Bivariate Time model R-squared:", round(summary(model2)$r.squared, 4), "\n")
cat("Multiple regression (StressSurvey + Time) R-squared:", round(summary(model3)$r.squared, 4), "\n")
cat("Multiple regression (Stress + Time) R-squared:", round(summary(model4)$r.squared, 4), "\n")

# Check for potential issues
cat("\nPotential Issues Analysis:\n")
cat("1. Residuals range:", round(range(residuals(model4)), 3), "\n")
cat("2. Cook's Distance max:", round(max(cooks.distance(model4)), 4), "\n")
cat("3. Leverage max:", round(max(hatvalues(model4)), 4), "\n")

# Check for outliers
studentized_residuals_true <- rstudent(model4)
outliers_true <- abs(studentized_residuals_true) > 2
cat("4. Potential outliers (|studentized residual| > 2):", sum(outliers_true), "observations\n")
if(sum(outliers_true) > 0) {
  cat("   Outlier observations:", which(outliers_true), "\n")
}
```

**This demonstrates the "gold standard" model - using the true variables (Stress and Time) gives perfect results with exactly the right coefficients (1.0 for Stress, 0.1 for Time) and 100% R-squared. This confirms our understanding of the true data-generating process and serves as the benchmark against which all other models can be compared.**

## Question 7

Compare the R-squared values and coefficient interpretations between the two multiple regression models. Do both models show statistical significance in all of their coefficient estimates? What does this tell you about the real-world implications of multiple regression results?

```{r}
# Create comprehensive model comparison
cat("=== MODEL COMPARISON ANALYSIS ===\n\n")

# Model 3: Anxiety ~ StressSurvey + Time
cat("MODEL 3: Anxiety ~ StressSurvey + Time\n")
cat("-----------------------------------\n")
summary_model3 <- summary(model3)
cat("R-squared:", round(summary_model3$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary_model3$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary_model3$sigma, 4), "\n\n")

cat("Coefficients:\n")
coef_table3 <- summary_model3$coefficients
print(round(coef_table3, 4))

cat("\nStatistical Significance:\n")
cat("StressSurvey: p-value =", round(coef_table3[2,4], 6), "→", ifelse(coef_table3[2,4] < 0.001, "Highly significant (***)", "Not significant"), "\n")
cat("Time: p-value =", round(coef_table3[3,4], 6), "→", ifelse(coef_table3[3,4] < 0.001, "Highly significant (***)", "Not significant"), "\n")

# Model 4: Anxiety ~ Stress + Time  
cat("\n\nMODEL 4: Anxiety ~ Stress + Time\n")
cat("------------------------------\n")
summary_model4 <- summary(model4)
cat("R-squared:", round(summary_model4$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary_model4$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary_model4$sigma, 4), "\n\n")

cat("Coefficients:\n")
coef_table4 <- summary_model4$coefficients
print(round(coef_table4, 4))

cat("\nStatistical Significance:\n")
cat("Stress: p-value =", round(coef_table4[2,4], 6), "→", ifelse(coef_table4[2,4] < 0.001, "Highly significant (***)", "Not significant"), "\n")
cat("Time: p-value =", round(coef_table4[3,4], 6), "→", ifelse(coef_table4[3,4] < 0.001, "Highly significant (***)", "Not significant"), "\n")

# Direct comparison
cat("\n\n=== DIRECT COMPARISON ===\n")
cat("R-squared Comparison:\n")
cat("  Model 3 (StressSurvey + Time):", round(summary_model3$r.squared, 4), "\n")
cat("  Model 4 (Stress + Time):", round(summary_model4$r.squared, 4), "\n")
cat("  Difference:", round(summary_model4$r.squared - summary_model3$r.squared, 4), "\n\n")

cat("Coefficient Comparison:\n")
cat("StressSurvey coefficient (Model 3):", round(coef_table3[2,1], 4), "\n")
cat("Stress coefficient (Model 4):", round(coef_table4[2,1], 4), "\n")
cat("Difference:", round(coef_table3[2,1] - coef_table4[2,1], 4), "\n\n")

cat("Time coefficient (Model 3):", round(coef_table3[3,1], 4), "\n")
cat("Time coefficient (Model 4):", round(coef_table4[3,1], 4), "\n")
cat("Difference:", round(coef_table3[3,1] - coef_table4[3,1], 4), "\n\n")

# True relationship comparison
cat("Comparison to True Relationship (Anxiety = Stress + 0.1 × Time):\n")
cat("Model 3 - StressSurvey:", round(coef_table3[2,1], 4), "(expected ~1.0), Time:", round(coef_table3[3,1], 4), "(expected 0.1)\n")
cat("Model 4 - Stress:", round(coef_table4[2,1], 4), "(expected 1.0), Time:", round(coef_table4[3,1], 4), "(expected 0.1)\n")

# Real-world implications analysis
cat("\n\n=== REAL-WORLD IMPLICATIONS ===\n")
cat("1. STATISTICAL SIGNIFICANCE:\n")
cat("   - Both models show highly significant coefficients (p < 0.001)\n")
cat("   - This demonstrates that significance alone cannot validate model quality\n")
cat("   - Wrong models can still produce 'statistically significant' results\n\n")

cat("2. R-SQUARED INTERPRETATION:\n")
cat("   - Model 3: 93.5% R-squared with severely biased coefficients\n")
cat("   - Model 4: 100% R-squared with perfect coefficients\n")
cat("   - High R-squared does not guarantee correct coefficient interpretation\n\n")

cat("3. COEFFICIENT BIAS:\n")
cat("   - Model 3: Time coefficient has wrong sign (-2.780 vs +0.1)\n")
cat("   - Model 4: Perfect coefficients matching true relationship\n")
cat("   - Multicollinearity creates misleading but 'significant' results\n\n")

cat("4. PRACTICAL IMPLICATIONS:\n")
cat("   - Using wrong variables (StressSurvey) leads to biased conclusions\n")
cat("   - Researchers must understand variable relationships before modeling\n")
cat("   - Model specification is more important than fit statistics\n")
cat("   - 'Garbage Can' regressions can produce impressive but wrong results\n")
```

**Both models show statistical significance, but the coefficient comparison reveals the problem: Model 3 has severely biased coefficients (Time coefficient has wrong sign: -2.780 vs +0.1) while Model 4 has perfect coefficients. This demonstrates that statistical significance alone cannot validate model quality - high R-squared and significant p-values can still produce completely misleading results when the underlying model is misspecified due to multicollinearity.**

## Question 8

Reflect on Real-World Implications: For each of the two multiple regression models, assume their respective outputs/conclusions were published in academic journals and then subsequently picked up by the popular press. What headline about time spent on social media and its effect on anxiety would you expect to see from a popular press outlet covering the first model? And what headline would you expect to see from a popular press outlet covering the second model? Assuming confirmation bias is real, which model is a typical parent going to believe? Which model will Facebook, Instagram, and TikTok executives prefer?

```{r}
# Real-world implications analysis
cat("=== REAL-WORLD IMPLICATIONS ANALYSIS ===\n\n")

# Model 3 Results (StressSurvey + Time)
cat("MODEL 3: Anxiety ~ StressSurvey + Time\n")
cat("-----------------------------------\n")
coef_model3 <- coef(model3)
cat("Key Finding: Time coefficient =", round(coef_model3[3], 4), "(NEGATIVE)\n")
cat("Interpretation: More time on social media REDUCES anxiety\n")
cat("Statistical Significance: Highly significant (p < 0.001)\n")
cat("R-squared: 93.5% (very impressive)\n\n")

# Model 4 Results (Stress + Time)
cat("MODEL 4: Anxiety ~ Stress + Time\n")
cat("-----------------------------------\n")
coef_model4 <- coef(model4)
cat("Key Finding: Time coefficient =", round(coef_model4[3], 4), "(POSITIVE)\n")
cat("Interpretation: More time on social media INCREASES anxiety\n")
cat("Statistical Significance: Highly significant (p < 0.001)\n")
cat("R-squared: 100% (perfect fit)\n\n")

# Popular Press Headlines
cat("=== POPULAR PRESS HEADLINES ===\n\n")
cat("HEADLINE FOR MODEL 3 (StressSurvey + Time):\n")
cat('"BREAKTHROUGH STUDY: More Time on Social Media Actually REDUCES Anxiety!"\n')
cat('"Researchers Find Surprising Benefit of Social Media Use"\n')
cat('"Study Shows Social Media Time Associated with Lower Anxiety Levels"\n\n')

cat("HEADLINE FOR MODEL 4 (Stress + Time):\n")
cat('"NEW RESEARCH CONFIRMS: Social Media Time Increases Anxiety"\n')
cat('"Study Links More Social Media Use to Higher Anxiety Levels"\n')
cat('"Research Validates Parental Concerns About Social Media Impact"\n\n')

# Stakeholder Analysis
cat("=== STAKEHOLDER ANALYSIS ===\n\n")
cat("1. TYPICAL PARENTS:\n")
cat("   - Will believe Model 4 (Stress + Time) due to confirmation bias\n")
cat("   - Aligns with their existing concerns about social media\n")
cat("   - Will dismiss Model 3 as 'obviously wrong' or 'industry-funded'\n")
cat("   - Likely to cite Model 4 when setting screen time limits\n\n")

cat("2. SOCIAL MEDIA EXECUTIVES (Facebook, Instagram, TikTok):\n")
cat("   - Will prefer Model 3 (StressSurvey + Time) results\n")
cat("   - Provides 'scientific evidence' that social media reduces anxiety\n")
cat("   - Can use for marketing: 'Research shows social media is good for mental health'\n")
cat("   - Will fund follow-up studies to replicate Model 3 findings\n")
cat("   - Will dismiss Model 4 as 'methodologically flawed'\n\n")

cat("3. RESEARCHERS AND ACADEMICS:\n")
cat("   - Should recognize Model 3 has multicollinearity problems\n")
cat("   - Should understand Model 4 represents true relationship\n")
cat("   - But may be influenced by funding sources or publication bias\n")
cat("   - Need to carefully examine model specification\n\n")

# Real-world consequences
cat("=== REAL-WORLD CONSEQUENCES ===\n\n")
cat("If Model 3 Results Were Published:\n")
cat("- Social media companies would heavily promote these findings\n")
cat("- Parents might relax screen time restrictions\n")
cat("- Policy makers might be less concerned about social media regulation\n")
cat("- Mental health professionals would face contradictory evidence\n")
cat("- Public trust in research could be undermined when truth emerges\n\n")

cat("If Model 4 Results Were Published:\n")
cat("- Social media companies would attack the methodology\n")
cat("- Parents would feel vindicated in their concerns\n")
cat("- Calls for social media regulation would increase\n")
cat("- Mental health interventions would focus more on screen time\n")
cat("- This aligns with broader research on social media and mental health\n\n")

# The danger of biased research
cat("=== THE DANGER OF BIASED RESEARCH ===\n\n")
cat("This example illustrates:\n")
cat("1. How statistical significance can be misleading\n")
cat("2. How different stakeholders interpret the same data differently\n")
cat("3. How confirmation bias affects which studies get attention\n")
cat("4. How industry funding can influence research conclusions\n")
cat("5. Why model specification matters more than fit statistics\n")
cat("6. How 'garbage can' regressions can have real-world consequences\n\n")

cat("CRITICAL INSIGHT: The same dataset can 'prove' opposite conclusions\n")
cat("depending on which variables are included in the model!\n")
```

**The same dataset produces opposite conclusions: Model 3 suggests social media reduces anxiety (Time coefficient = -2.780), while Model 4 shows it increases anxiety (Time coefficient = +0.1). Parents will believe Model 4 due to confirmation bias, while social media executives will prefer Model 3 for marketing. This demonstrates how model specification choices can have profound real-world consequences, affecting policy decisions and public health interventions.**

## Question 9

Avoiding Misleading Statistical Significance: Reflect on this tip to avoid being misled by statistically significant results: splitting the sample into meaningful subsets ("statistical regimes"), and using graphical diagnostics for linearity rather than blind reliance on "canned" regressions. Apply this approach to multiple regression of Anxiety on both StressSurvey and Time by analyzing a smartly chosen subset of the data. What specific subset did you choose and why? Did you get results that are both statistically significant and close to the true relationship?

```{r}
# Avoiding misleading statistical significance through subset analysis
cat("=== AVOIDING MISLEADING STATISTICAL SIGNIFICANCE ===\n\n")

# First, let's examine the data structure to identify meaningful subsets
cat("EXAMINING DATA STRUCTURE FOR MEANINGFUL SUBSETS:\n")
cat("----------------------------------------------\n")
cat("Unique Stress values:", sort(unique(observDF$Stress)), "\n")
cat("Unique StressSurvey values:", sort(unique(observDF$StressSurvey)), "\n")
cat("Unique Time values:", sort(unique(observDF$Time)), "\n\n")

# Create a subset based on "low stress" regime vs "high stress" regime
# This makes theoretical sense - different stress levels might have different relationships
cat("SUBSET SELECTION STRATEGY:\n")
cat("-------------------------\n")
cat("I'll split the data into two 'statistical regimes':\n")
cat("1. LOW STRESS REGIME: Stress ≤ 2 (observations 1-9)\n")
cat("2. HIGH STRESS REGIME: Stress > 2 (observations 10-15)\n\n")
cat("Rationale: Different stress levels might exhibit different anxiety-time relationships\n")
cat("This creates meaningful subsets rather than arbitrary splits\n\n")

# Create subsets
low_stress_subset <- observDF[observDF$Stress <= 2, ]
high_stress_subset <- observDF[observDF$Stress > 2, ]

cat("SUBSET SIZES:\n")
cat("Low stress subset: n =", nrow(low_stress_subset), "observations\n")
cat("High stress subset: n =", nrow(high_stress_subset), "observations\n\n")

# Analyze low stress subset
cat("=== LOW STRESS REGIME ANALYSIS ===\n")
cat("Stress levels: 0, 1, 2\n")
cat("Sample size:", nrow(low_stress_subset), "\n\n")

model_low <- lm(Anxiety ~ StressSurvey + Time, data = low_stress_subset)
summary_low <- summary(model_low)

cat("Multiple Regression Results (Low Stress):\n")
print(round(summary_low$coefficients, 4))

cat("\nFit Statistics:\n")
cat("R-squared:", round(summary_low$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary_low$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary_low$sigma, 4), "\n")

# Check statistical significance
coef_low <- summary_low$coefficients
cat("\nStatistical Significance:\n")
cat("StressSurvey: p-value =", round(coef_low[2,4], 6), "→", ifelse(coef_low[2,4] < 0.05, "Significant", "Not significant"), "\n")
cat("Time: p-value =", round(coef_low[3,4], 6), "→", ifelse(coef_low[3,4] < 0.05, "Significant", "Not significant"), "\n")

# Analyze high stress subset
cat("\n\n=== HIGH STRESS REGIME ANALYSIS ===\n")
cat("Stress levels: 8, 12\n")
cat("Sample size:", nrow(high_stress_subset), "\n\n")

model_high <- lm(Anxiety ~ StressSurvey + Time, data = high_stress_subset)
summary_high <- summary(model_high)

cat("Multiple Regression Results (High Stress):\n")
print(round(summary_high$coefficients, 4))

cat("\nFit Statistics:\n")
cat("R-squared:", round(summary_high$r.squared, 4), "\n")
cat("Adjusted R-squared:", round(summary_high$adj.r.squared, 4), "\n")
cat("Residual Standard Error:", round(summary_high$sigma, 4), "\n")

# Check statistical significance
coef_high <- summary_high$coefficients
cat("\nStatistical Significance:\n")
cat("StressSurvey: p-value =", round(coef_high[2,4], 6), "→", ifelse(coef_high[2,4] < 0.05, "Significant", "Not significant"), "\n")
cat("Time: p-value =", round(coef_high[3,4], 6), "→", ifelse(coef_high[3,4] < 0.05, "Significant", "Not significant"), "\n")

# Graphical diagnostics for linearity
cat("\n\n=== GRAPHICAL DIAGNOSTICS FOR LINEARITY ===\n")
cat("Creating diagnostic plots to assess linearity assumptions...\n\n")

# Create diagnostic plots
par(mfrow = c(2, 2))

# Low stress regime plots
plot(model_low, which = 1, main = "Low Stress: Residuals vs Fitted")
plot(model_low, which = 2, main = "Low Stress: Q-Q Plot")
plot(model_low, which = 3, main = "Low Stress: Scale-Location")
plot(model_low, which = 5, main = "Low Stress: Cook's Distance")

# Reset plotting parameters
par(mfrow = c(1, 1))

# Compare to true relationship
cat("COMPARISON TO TRUE RELATIONSHIP (Anxiety = Stress + 0.1 × Time):\n")
cat("----------------------------------------------------------------\n")
cat("Low Stress Regime:\n")
cat("  Expected StressSurvey coefficient: ~1.0 (proxy for Stress)\n")
cat("  Expected Time coefficient: 0.1\n")
cat("  Estimated StressSurvey coefficient:", round(coef_low[2,1], 4), "\n")
cat("  Estimated Time coefficient:", round(coef_low[3,1], 4), "\n\n")

cat("High Stress Regime:\n")
cat("  Expected StressSurvey coefficient: ~1.0 (proxy for Stress)\n")
cat("  Expected Time coefficient: 0.1\n")
cat("  Estimated StressSurvey coefficient:", round(coef_high[2,1], 4), "\n")
cat("  Estimated Time coefficient:", round(coef_high[3,1], 4), "\n\n")

# Summary of findings
cat("=== SUMMARY OF SUBSET ANALYSIS ===\n")
cat("1. SUBSET CHOICE: Split by stress level (≤2 vs >2) to create meaningful regimes\n")
cat("2. LOW STRESS REGIME: Smaller sample, potential multicollinearity issues\n")
cat("3. HIGH STRESS REGIME: Very small sample, limited variation\n")
cat("4. GRAPHICAL DIAGNOSTICS: Reveal potential issues with linearity assumptions\n")
cat("5. STATISTICAL SIGNIFICANCE: May be misleading due to small sample sizes\n\n")

cat("KEY INSIGHTS:\n")
cat("- Subsetting can reduce multicollinearity problems\n")
cat("- But small samples can create new statistical issues\n")
cat("- Graphical diagnostics are crucial for assessing model assumptions\n")
cat("- Statistical significance alone is not sufficient for model validation\n")
cat("- True relationship may be obscured by data limitations in subsets\n")
```

**Subset analysis successfully recovered the true relationship: both low-stress and high-stress regimes correctly estimated the Time coefficient as 0.1, matching the true relationship. By splitting into meaningful "statistical regimes" (stress ≤2 vs >2), we reduced multicollinearity problems and revealed the underlying truth that was obscured in the full model. This demonstrates how thoughtful subsetting and graphical diagnostics can help avoid being misled by "canned" regression results, though small sample sizes create new statistical challenges.** 